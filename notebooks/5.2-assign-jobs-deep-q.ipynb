{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign jobs directly with deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare programming environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "from tensorforce.environments import Environment\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "from tensorforce.agents import Agent\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.simulation import assign_priority_edd, assign_priority_mdd, assign_priority_spt, assign_priority_srpt, assign_priority_lpt, assign_priority_cr, assign_priority_ds, assign_priority_fifo, select_machine_winq\n",
    "from src.models import AssignEnvironment, hyperparameter_tuning_assign_deepq, run_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data required for simulation\n",
    "product_types_df = pd.read_csv(\"../data/external/product_types.csv\")\n",
    "product_types_df = product_types_df[product_types_df.id != 2]\n",
    "with open(r\"../data/interim/sim_data.pickle\", \"rb\") as output_file:\n",
    "    orders_df = pickle.load(output_file)\n",
    "machines_df = pd.read_csv(\"../data/external/machine.csv\")\n",
    "machines_df = machines_df[machines_df.product_type_id != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for simulation\n",
    "priority_rules = [assign_priority_edd, assign_priority_spt, assign_priority_srpt, assign_priority_fifo, assign_priority_cr,\n",
    "                  assign_priority_mdd, assign_priority_lpt, assign_priority_ds]\n",
    "SIMULATION_START = mktime(datetime(2022, 11, 14, 5, 0, 0).timetuple()) * 1000\n",
    "due_date_range_list = [(3, 10), (5, 14), (7, 21)]\n",
    "number_orders_start_list = [80, 90, 100, 110]\n",
    "average_count_new_orders_list = [80, 90, 100, 110]\n",
    "worker_list =  [40, 50, 60, 70]\n",
    "random_states = [7, 42, 66, 97, 108]\n",
    "random_states_evaluation = [100, 101, 102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning environment\n",
    "environment = Environment.create(\n",
    "    environment=AssignEnvironment(product_types_df, machines_df, orders_df,priority_rule=assign_priority_edd,\n",
    "                 simulation_start=SIMULATION_START, allocation_rule=select_machine_winq, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for deep q learning\n",
    "params_q = {\"batch_size\": [5, 10, 20, 30, 50, 100],\n",
    "            \"update_frequency\": [0.25, 0.5, 1.0],\n",
    "            \"horizon\": [5, 10, 20, 30, 50, 100],\n",
    "            \"discount\": [0.9, 0.7, 0.5, 0.01],\n",
    "            \"return_processing\": [dict(type='exponential_normalization', decay=0.9), None],\n",
    "            \"reward_processing\": [dict(type='exponential_normalization', decay=0.9), None],\n",
    "            \"state_preprocessing\": [dict(type='exponential_normalization', decay=0.9),  None],\n",
    "            \"target_update_weight\": [0.7, 1.0],\n",
    "            \"l2_regularization\": [0.01, 0.0],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute hyperparameter search\n",
    "# tuning_res = hyperparameter_tuning_assign_deepq(environment, params_q, 1, due_date_range_list,\n",
    "#                                               number_orders_start_list, average_count_new_orders_list,\n",
    "#                                               worker_list, [42], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results of hyperparameter search\n",
    "# tuning_res.to_csv(\"../data/processed/hyperparameter_search/results_hyperparameter_select_deep_q_learning.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hyperparameter results\n",
    "tuning_res = pd.read_csv(\"../data/processed/hyperparameter_search/results_hyperparameter_select_deep_q_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "     batch_size  update_frequency  horizon  discount return_processing  \\\n26            5               0.5       30      0.90               NaN   \n63            5               0.5       30      0.90               NaN   \n38           30               0.5        5      0.90               NaN   \n161           5               1.0       30      0.50               NaN   \n179          30               0.5        5      0.90               NaN   \n..          ...               ...      ...       ...               ...   \n108           5               0.5       30      0.90               NaN   \n34            5               1.0        5      0.01               NaN   \n152          30               1.0       30      0.01               NaN   \n18            5               0.5       30      0.01               NaN   \n195           5               1.0       30      0.90               NaN   \n\n                                   state_preprocessing reward_processing  \\\n26                                                 NaN               NaN   \n63                                                 NaN               NaN   \n38                                                 NaN               NaN   \n161                                                NaN               NaN   \n179                                                NaN               NaN   \n..                                                 ...               ...   \n108  {'type': 'exponential_normalization', 'decay':...               NaN   \n34   {'type': 'exponential_normalization', 'decay':...               NaN   \n152  {'type': 'exponential_normalization', 'decay':...               NaN   \n18   {'type': 'exponential_normalization', 'decay':...               NaN   \n195  {'type': 'exponential_normalization', 'decay':...               NaN   \n\n     target_update_weight  l2_regularization  reward_training_mean  \\\n26                    0.7               0.01           8176.041143   \n63                    0.7               0.00           8167.411602   \n38                    1.0               0.01           8165.938390   \n161                   0.7               0.01           8153.888962   \n179                   0.7               0.00           8121.865734   \n..                    ...                ...                   ...   \n108                   0.7               0.00           6989.503386   \n34                    1.0               0.00           6950.360795   \n152                   0.7               0.01           6863.783064   \n18                    1.0               0.01           6821.040351   \n195                   1.0               0.01           6769.845688   \n\n     reward_training_var  reward_evaluation_mean  reward_evaluation_var  \\\n26          2.227054e+07             6119.041612           2.477160e+07   \n63          2.234050e+07             6127.736844           2.438002e+07   \n38          2.214403e+07             6174.388806           2.537082e+07   \n161         2.247067e+07             6099.959301           2.564236e+07   \n179         2.231287e+07             6109.413542           2.601128e+07   \n..                   ...                     ...                    ...   \n108         2.536119e+07             5213.493603           2.947091e+07   \n34          2.569203e+07             5275.553131           2.868677e+07   \n152         2.574048e+07             5131.338146           2.876212e+07   \n18          2.530350e+07             5065.431576           2.920767e+07   \n195         2.522455e+07             5037.432500           2.921786e+07   \n\n                                             reward_df  \n26         episode  day       reward due_date_range...  \n63         episode  day       reward due_date_range...  \n38         episode  day       reward due_date_range...  \n161        episode  day        reward due_date_rang...  \n179        episode  day        reward due_date_rang...  \n..                                                 ...  \n108        episode  day       reward due_date_range...  \n34         episode  day       reward due_date_range...  \n152        episode  day       reward due_date_range...  \n18         episode  day       reward due_date_range...  \n195        episode  day        reward due_date_rang...  \n\n[200 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>batch_size</th>\n      <th>update_frequency</th>\n      <th>horizon</th>\n      <th>discount</th>\n      <th>return_processing</th>\n      <th>state_preprocessing</th>\n      <th>reward_processing</th>\n      <th>target_update_weight</th>\n      <th>l2_regularization</th>\n      <th>reward_training_mean</th>\n      <th>reward_training_var</th>\n      <th>reward_evaluation_mean</th>\n      <th>reward_evaluation_var</th>\n      <th>reward_df</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26</th>\n      <td>5</td>\n      <td>0.5</td>\n      <td>30</td>\n      <td>0.90</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.7</td>\n      <td>0.01</td>\n      <td>8176.041143</td>\n      <td>2.227054e+07</td>\n      <td>6119.041612</td>\n      <td>2.477160e+07</td>\n      <td>episode  day       reward due_date_range...</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>5</td>\n      <td>0.5</td>\n      <td>30</td>\n      <td>0.90</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.7</td>\n      <td>0.00</td>\n      <td>8167.411602</td>\n      <td>2.234050e+07</td>\n      <td>6127.736844</td>\n      <td>2.438002e+07</td>\n      <td>episode  day       reward due_date_range...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>30</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>0.90</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.01</td>\n      <td>8165.938390</td>\n      <td>2.214403e+07</td>\n      <td>6174.388806</td>\n      <td>2.537082e+07</td>\n      <td>episode  day       reward due_date_range...</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>5</td>\n      <td>1.0</td>\n      <td>30</td>\n      <td>0.50</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.7</td>\n      <td>0.01</td>\n      <td>8153.888962</td>\n      <td>2.247067e+07</td>\n      <td>6099.959301</td>\n      <td>2.564236e+07</td>\n      <td>episode  day        reward due_date_rang...</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>30</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>0.90</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.7</td>\n      <td>0.00</td>\n      <td>8121.865734</td>\n      <td>2.231287e+07</td>\n      <td>6109.413542</td>\n      <td>2.601128e+07</td>\n      <td>episode  day        reward due_date_rang...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>5</td>\n      <td>0.5</td>\n      <td>30</td>\n      <td>0.90</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>0.7</td>\n      <td>0.00</td>\n      <td>6989.503386</td>\n      <td>2.536119e+07</td>\n      <td>5213.493603</td>\n      <td>2.947091e+07</td>\n      <td>episode  day       reward due_date_range...</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>5</td>\n      <td>1.0</td>\n      <td>5</td>\n      <td>0.01</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>6950.360795</td>\n      <td>2.569203e+07</td>\n      <td>5275.553131</td>\n      <td>2.868677e+07</td>\n      <td>episode  day       reward due_date_range...</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>30</td>\n      <td>1.0</td>\n      <td>30</td>\n      <td>0.01</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>0.7</td>\n      <td>0.01</td>\n      <td>6863.783064</td>\n      <td>2.574048e+07</td>\n      <td>5131.338146</td>\n      <td>2.876212e+07</td>\n      <td>episode  day       reward due_date_range...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>5</td>\n      <td>0.5</td>\n      <td>30</td>\n      <td>0.01</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.01</td>\n      <td>6821.040351</td>\n      <td>2.530350e+07</td>\n      <td>5065.431576</td>\n      <td>2.920767e+07</td>\n      <td>episode  day       reward due_date_range...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>5</td>\n      <td>1.0</td>\n      <td>30</td>\n      <td>0.90</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>0.01</td>\n      <td>6769.845688</td>\n      <td>2.522455e+07</td>\n      <td>5037.432500</td>\n      <td>2.921786e+07</td>\n      <td>episode  day        reward due_date_rang...</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best performing combination\n",
    "tuning_res.sort_values([\"reward_training_mean\", \"reward_evaluation_mean\"], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No min_value bound specified for state.\n"
     ]
    }
   ],
   "source": [
    "# Define agent based on hyperparameter results\n",
    "agent = Agent.create(\n",
    "    agent='dqn', environment=environment, memory=200, batch_size=20,\n",
    "    summarizer=dict(\n",
    "        directory='summaries/assign/deepq',\n",
    "        summaries=[\"action-value\", \"entropy\", \"graph\", \"kl-divergence\", \"loss\", \"parameters\", \"reward\", \"update-norm\",\n",
    "                   \"updates\", \"variables\"]\n",
    "    ),\n",
    "    update_frequency=0.25, learning_rate=0.001, horizon=30, discount=0.5, return_processing=None, reward_processing=None,\n",
    "    state_preprocessing=None,\n",
    "    target_update_weight=0.7, exploration=0.2, l2_regularization=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent for 100 episodes to train it\n",
    "# Duration ~ 35 hours\n",
    "# rewards_list = run_agent(agent, environment, due_date_range_list,number_orders_start_list,\n",
    "#                         average_count_new_orders_list, worker_list, random_states, episodes = 40, evaluate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "# agent.save(directory=\"../models/assign_deep_q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the model\n",
    "# Removed as the file has a size of over 100 GB\n",
    "# %tensorboard --logdir summaries/assign/deepq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent for evaluation\n",
    "# Duration ~ 20 minutes\n",
    "# rewards_list_evaluation = run_agent(agent, environment, due_date_range_list,number_orders_start_list,\n",
    "#                                    average_count_new_orders_list, worker_list, random_states_evaluation, episodes = 1, evaluate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# rewards_list_evaluation.to_csv(\"../data/processed/evaluation/simulation_results_validation_assign_deepq.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Load results\n",
    "rewards_list_evaluation = pd.read_csv(\"../data/processed/evaluation/simulation_results_validation_assign_deepq.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "9693.44302001655"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean reward by summing all reward and dividing through the number of days\n",
    "rewards_list_evaluation[\"reward\"].sum() / 17280"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
