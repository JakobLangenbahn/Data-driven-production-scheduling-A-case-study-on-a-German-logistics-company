{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign jobs directly using actor critic reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare programming environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "from tensorforce.agents import Agent\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.simulation import assign_priority_edd, assign_priority_mdd, assign_priority_spt, assign_priority_srpt, assign_priority_lpt, assign_priority_cr, assign_priority_ds, assign_priority_fifo, select_machine_winq\n",
    "from src.models import AssignEnvironment, hyperparameter_tuning_assign_ac, run_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data required for simulation\n",
    "product_types_df = pd.read_csv(\"../data/external/product_types.csv\")\n",
    "product_types_df = product_types_df[product_types_df.id != 2]\n",
    "with open(r\"../data/interim/sim_data.pickle\", \"rb\") as output_file:\n",
    "    orders_df = pickle.load(output_file)\n",
    "machines_df = pd.read_csv(\"../data/external/machine.csv\")\n",
    "machines_df = machines_df[machines_df.product_type_id != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for simulation\n",
    "priority_rules = [assign_priority_edd, assign_priority_spt, assign_priority_srpt, assign_priority_fifo, assign_priority_cr,\n",
    "                  assign_priority_mdd, assign_priority_lpt, assign_priority_ds]\n",
    "SIMULATION_START = mktime(datetime(2022, 11, 14, 5, 0, 0).timetuple()) * 1000\n",
    "due_date_range_list = [(3, 10), (5, 14), (7, 21)]\n",
    "number_orders_start_list = [80, 90, 100, 110]\n",
    "average_count_new_orders_list = [80, 90, 100, 110]\n",
    "worker_list =  [40, 50, 60, 70]\n",
    "random_states = [7, 42, 66, 97, 108]\n",
    "random_states_evaluation = [100, 101, 102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.create(\n",
    "    environment=AssignEnvironment(product_types_df, machines_df, orders_df,priority_rule=assign_priority_edd,\n",
    "                 simulation_start=SIMULATION_START, allocation_rule=select_machine_winq, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for deep q learning\n",
    "params_ac = {\"batch_size\": [5, 10, 20, 30, 50, 100],\n",
    "            \"update_frequency\": [0.25, 0.5, 1.0],\n",
    "            \"horizon\": [5, 10, 20, 30, 50, 100],\n",
    "            \"discount\": [0.9, 0.7, 0.5, 0.01],\n",
    "            \"return_processing\": [dict(type='exponential_normalization', decay=0.9), None],\n",
    "            \"reward_processing\": [dict(type='exponential_normalization', decay=0.9), None],\n",
    "            \"state_preprocessing\": [dict(type='exponential_normalization', decay=0.9),  None],\n",
    "            \"target_update_weight\": [0.7, 1.0],\n",
    "            \"l2_regularization\": [0.01, 0.0],\n",
    "            \"likelihood_ratio_clipping\": [0.1, 0.2, 0.3],\n",
    "            \"entropy_regularization\": [0.0, 0.01],\n",
    "            \"critic_optimizer\": [0.5,0.9, 1.0]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute hyperparameter search\n",
    "# tuning_res = hyperparameter_tuning_assign_ac(environment, params_ac, 1, due_date_range_list,\n",
    "#                                               number_orders_start_list, average_count_new_orders_list,\n",
    "#                                               worker_list, [42], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results of hyperparameter search\n",
    "# tuning_res.to_csv(\"../data/processed/hyperparameter_search/results_hyperparameter_select_deep_q_learning.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hyperparameter results\n",
    "tuning_res = pd.read_csv(\"../data/processed/hyperparameter_search/results_hyperparameter_assign_actor-critic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show best performing combination\n",
    "tuning_res.sort_values([\"reward_training_mean\", \"reward_evaluation_mean\"], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse tuning results\n",
    "tuning_res.groupby([\"batch_size\", \"update_frequency\", \"horizon\", \"discount\"])[\"reward_training_mean\", \"reward_evaluation_mean\"].mean().sort_values(\"reward_evaluation_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent based on hyperparameter results\n",
    "agent = Agent.create(\n",
    "    agent='ac', environment=environment, memory=11000, max_episode_timesteps=1000, batch_size=50,\n",
    "    summarizer=dict(\n",
    "        directory='summaries/assign/actor_critic/',\n",
    "        summaries=[\"action-value\", \"entropy\", \"graph\", \"kl-divergence\", \"loss\", \"parameters\", \"reward\", \"update-norm\",\n",
    "                   \"updates\", \"variables\"]\n",
    "    ),\n",
    "    update_frequency=0.5, learning_rate=0.001, horizon=30, discount=0.9, return_processing=None, reward_processing=None,\n",
    "    state_preprocessing=None, entropy_regularization = 0.01,critic_optimizer = 0.5, exploration=0.2, l2_regularization=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent for 100 episodes to train it\n",
    "# Duration ~ 26 hours\n",
    "# rewards_list = run_agent(agent, environment, due_date_range_list,number_orders_start_list,\n",
    "#                         average_count_new_orders_list, worker_list, random_states, episodes = 25, evaluate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "# agent.save(directory=\"../models/assign_actor_critic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir summaries/assign/actor_critic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent for evaluation\n",
    "# Duration ~ 22 minutes\n",
    "# rewards_list_evaluation = run_agent(agent, environment, due_date_range_list,number_orders_start_list,\n",
    "#                                    average_count_new_orders_list, worker_list, random_states_evaluation,\n",
    "#                                    episodes = 1, evaluate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# rewards_list_evaluation.to_csv(\"../data/processed/evaluation/simulation_results_validation_assign_actor_critic.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load results\n",
    "rewards_list_evaluation = pd.read_csv(\"../data/processed/evaluation/simulation_results_validation_assign_actor_critic.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate mean reward by summing all reward and dividing through the number of days\n",
    "rewards_list_evaluation[\"reward\"].sum() / 17280"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
