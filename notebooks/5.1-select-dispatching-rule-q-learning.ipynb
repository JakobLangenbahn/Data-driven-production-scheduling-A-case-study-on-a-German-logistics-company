{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select dispatching rule using Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare programming environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from mushroom_rl.core import Environment\n",
    "from minisom import MiniSom\n",
    "from mushroom_rl.algorithms.value import QLearning\n",
    "from mushroom_rl.core import MDPInfo, Core\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.utils.parameters import Parameter\n",
    "from mushroom_rl.utils.spaces import Discrete\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from time import mktime\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.simulation import assign_priority_edd, assign_priority_mdd, assign_priority_spt, assign_priority_srpt, assign_priority_lpt, assign_priority_cr, assign_priority_ds, assign_priority_fifo, select_machine_winq\n",
    "from src.models import PlantEnv, hyperparameter_tuning_mushroom, run_agent, train_som"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for training SOM\n",
    "with open(r\"../data/processed/data_processed_classification.pickle\", \"rb\") as output_file:\n",
    "        data = pickle.load(output_file)\n",
    "        data = pd.merge(data, pd.get_dummies(data[\"priority_rule_start\"]), left_index = True, right_index = True)\n",
    "        data.drop(\"priority_rule_start\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data required for simulation\n",
    "product_types_df = pd.read_csv(\"../data/external/product_types.csv\")\n",
    "product_types_df = product_types_df[product_types_df.id != 2]\n",
    "with open(r\"../data/interim/sim_data.pickle\", \"rb\") as output_file:\n",
    "    orders_df = pickle.load(output_file)\n",
    "machines_df = pd.read_csv(\"../data/external/machine.csv\")\n",
    "machines_df = machines_df[machines_df.product_type_id != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for simulation\n",
    "RANDOM_STATE = 42\n",
    "priority_rules = [assign_priority_edd, assign_priority_spt, assign_priority_srpt, assign_priority_fifo, assign_priority_cr,\n",
    "                  assign_priority_mdd, assign_priority_lpt, assign_priority_ds]\n",
    "SIMULATION_START = mktime(datetime(2022, 11, 14, 5, 0, 0).timetuple()) * 1000\n",
    "due_date_range_list = [(3, 10), (5, 14), (7, 21)]\n",
    "number_orders_start_list = [80, 90, 100, 110]\n",
    "average_count_new_orders_list = [80, 90, 100, 110]\n",
    "worker_list =  [40, 50, 60, 70]\n",
    "random_states = [7, 42, 66, 97, 108]\n",
    "random_states_evaluation = [100, 101, 102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid\n",
    "params_q = {\"epsilon_param\": [0.25, 0.5, 0.9],\n",
    "            \"learning_rate\": [0.2, 0.6, 1.0],\n",
    "            \"number_of_states\": [16, 32, 64],\n",
    "            \"sigma\": [0.5, 1.0, 1.5, 2.0],\n",
    "            \"learning_rate_som\": [0.1, 0.5, 0.9]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "# tuning_res =  hyperparameter_tuning_mushroom(params_q, 20, data, product_types_df, machines_df, orders_df, SIMULATION_START, priority_rules, due_date_range_list, number_orders_start_list, average_count_new_orders_list, worker_list, random_states, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results of hyperparameter search\n",
    "# tuning_res.to_csv(\"../data/processed/results_hyperparameter_select_q_learning.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hyperparameter results\n",
    "tuning_res = pd.read_csv(\"../data/processed/hyperparameter_search/results_hyperparameter_select_q_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    epsilon_param  learning_rate  number_of_states  sigma  learning_rate_som  \\\n19           0.50            0.2                64    1.0                0.5   \n4            0.25            0.6                16    1.0                0.9   \n7            0.25            0.6                32    0.5                0.5   \n17           0.25            0.2                64    2.0                0.1   \n1            0.25            0.6                64    1.0                0.5   \n15           0.25            0.6                64    0.5                0.1   \n18           0.50            0.6                64    1.0                0.5   \n12           0.25            0.6                64    1.5                0.5   \n2            0.50            1.0                64    0.5                0.1   \n9            0.25            1.0                64    1.0                0.1   \n5            0.50            0.2                32    2.0                0.9   \n13           0.25            0.2                16    1.5                0.1   \n16           0.90            0.2                16    1.0                0.1   \n0            0.90            0.2                16    1.5                0.1   \n10           0.50            0.2                64    2.0                0.5   \n11           0.90            0.2                32    2.0                0.9   \n14           0.90            0.2                32    1.5                0.9   \n3            0.50            0.6                32    1.5                0.1   \n6            0.90            0.6                16    1.5                0.9   \n8            0.50            0.6                32    1.0                0.9   \n\n    reward_evaluation_mean  reward_evaluation_var  \\\n19             7595.498331           2.393337e+07   \n4              7495.951348           2.526382e+07   \n7              7479.991394           2.408755e+07   \n17             7444.201133           2.409298e+07   \n1              7421.791986           2.411351e+07   \n15             7403.157619           2.409854e+07   \n18             7384.570955           2.447151e+07   \n12             7367.769524           2.383983e+07   \n2              7328.841823           2.496098e+07   \n9              7312.888423           2.300382e+07   \n5              7273.905213           2.344808e+07   \n13             7248.070456           2.427872e+07   \n16             7230.088093           2.463671e+07   \n0              7164.507328           2.457015e+07   \n10             7147.687993           2.408878e+07   \n11             7134.577447           2.489847e+07   \n14             7127.264593           2.470544e+07   \n3              7117.877969           2.375120e+07   \n6              7117.486374           2.494366e+07   \n8              6993.027652           2.601784e+07   \n\n                                            reward_df  \n19        state_previous action        reward stat...  \n4         state_previous action        reward stat...  \n7         state_previous action        reward stat...  \n17        state_previous action       reward state...  \n1         state_previous action       reward state...  \n15        state_previous action        reward stat...  \n18        state_previous action       reward state...  \n12        state_previous action        reward stat...  \n2         state_previous action        reward stat...  \n9         state_previous action        reward stat...  \n5         state_previous action        reward stat...  \n13        state_previous action        reward stat...  \n16        state_previous action        reward stat...  \n0         state_previous action        reward stat...  \n10        state_previous action        reward stat...  \n11        state_previous action        reward stat...  \n14        state_previous action        reward stat...  \n3         state_previous action        reward stat...  \n6         state_previous action        reward stat...  \n8         state_previous action        reward stat...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>epsilon_param</th>\n      <th>learning_rate</th>\n      <th>number_of_states</th>\n      <th>sigma</th>\n      <th>learning_rate_som</th>\n      <th>reward_evaluation_mean</th>\n      <th>reward_evaluation_var</th>\n      <th>reward_df</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19</th>\n      <td>0.50</td>\n      <td>0.2</td>\n      <td>64</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>7595.498331</td>\n      <td>2.393337e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.25</td>\n      <td>0.6</td>\n      <td>16</td>\n      <td>1.0</td>\n      <td>0.9</td>\n      <td>7495.951348</td>\n      <td>2.526382e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.25</td>\n      <td>0.6</td>\n      <td>32</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>7479.991394</td>\n      <td>2.408755e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.25</td>\n      <td>0.2</td>\n      <td>64</td>\n      <td>2.0</td>\n      <td>0.1</td>\n      <td>7444.201133</td>\n      <td>2.409298e+07</td>\n      <td>state_previous action       reward state...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.25</td>\n      <td>0.6</td>\n      <td>64</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>7421.791986</td>\n      <td>2.411351e+07</td>\n      <td>state_previous action       reward state...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.25</td>\n      <td>0.6</td>\n      <td>64</td>\n      <td>0.5</td>\n      <td>0.1</td>\n      <td>7403.157619</td>\n      <td>2.409854e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.50</td>\n      <td>0.6</td>\n      <td>64</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>7384.570955</td>\n      <td>2.447151e+07</td>\n      <td>state_previous action       reward state...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.25</td>\n      <td>0.6</td>\n      <td>64</td>\n      <td>1.5</td>\n      <td>0.5</td>\n      <td>7367.769524</td>\n      <td>2.383983e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.50</td>\n      <td>1.0</td>\n      <td>64</td>\n      <td>0.5</td>\n      <td>0.1</td>\n      <td>7328.841823</td>\n      <td>2.496098e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.25</td>\n      <td>1.0</td>\n      <td>64</td>\n      <td>1.0</td>\n      <td>0.1</td>\n      <td>7312.888423</td>\n      <td>2.300382e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.50</td>\n      <td>0.2</td>\n      <td>32</td>\n      <td>2.0</td>\n      <td>0.9</td>\n      <td>7273.905213</td>\n      <td>2.344808e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.25</td>\n      <td>0.2</td>\n      <td>16</td>\n      <td>1.5</td>\n      <td>0.1</td>\n      <td>7248.070456</td>\n      <td>2.427872e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.90</td>\n      <td>0.2</td>\n      <td>16</td>\n      <td>1.0</td>\n      <td>0.1</td>\n      <td>7230.088093</td>\n      <td>2.463671e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.90</td>\n      <td>0.2</td>\n      <td>16</td>\n      <td>1.5</td>\n      <td>0.1</td>\n      <td>7164.507328</td>\n      <td>2.457015e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.50</td>\n      <td>0.2</td>\n      <td>64</td>\n      <td>2.0</td>\n      <td>0.5</td>\n      <td>7147.687993</td>\n      <td>2.408878e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.90</td>\n      <td>0.2</td>\n      <td>32</td>\n      <td>2.0</td>\n      <td>0.9</td>\n      <td>7134.577447</td>\n      <td>2.489847e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.90</td>\n      <td>0.2</td>\n      <td>32</td>\n      <td>1.5</td>\n      <td>0.9</td>\n      <td>7127.264593</td>\n      <td>2.470544e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.50</td>\n      <td>0.6</td>\n      <td>32</td>\n      <td>1.5</td>\n      <td>0.1</td>\n      <td>7117.877969</td>\n      <td>2.375120e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.90</td>\n      <td>0.6</td>\n      <td>16</td>\n      <td>1.5</td>\n      <td>0.9</td>\n      <td>7117.486374</td>\n      <td>2.494366e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.50</td>\n      <td>0.6</td>\n      <td>32</td>\n      <td>1.0</td>\n      <td>0.9</td>\n      <td>6993.027652</td>\n      <td>2.601784e+07</td>\n      <td>state_previous action        reward stat...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best performing combination\n",
    "tuning_res.sort_values([\"reward_evaluation_mean\"], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "epsilon_param\n0.90    7154.784767\n0.50    7263.058562\n0.25    7396.727736\nName: reward_evaluation_mean, dtype: float64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse tuning results\n",
    "tuning_res.groupby([\"epsilon_param\"])[\"reward_evaluation_mean\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "learning_rate\n0.2    7262.866732\n0.6    7309.069425\n1.0    7320.865123\nName: reward_evaluation_mean, dtype: float64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse tuning results\n",
    "tuning_res.groupby([\"learning_rate\"])[\"reward_evaluation_mean\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "number_of_states\n32    7187.774045\n16    7251.220720\n64    7378.489754\nName: reward_evaluation_mean, dtype: float64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse tuning results\n",
    "tuning_res.groupby([\"number_of_states\"])[\"reward_evaluation_mean\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "sigma\n1.5    7190.496041\n2.0    7250.092947\n1.0    7347.688113\n0.5    7403.996945\nName: reward_evaluation_mean, dtype: float64"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse tuning results\n",
    "tuning_res.groupby([\"sigma\"])[\"reward_evaluation_mean\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "learning_rate_som\n0.9    7190.368771\n0.1    7281.204106\n0.5    7399.551697\nName: reward_evaluation_mean, dtype: float64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse tuning results\n",
    "tuning_res.groupby([\"learning_rate_som\"])[\"reward_evaluation_mean\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 10000 / 10000 ] 100% - 0:00:00 left \n",
      " quantization error: 3.2128769427259583\n"
     ]
    }
   ],
   "source": [
    "# Train final som\n",
    "som, normalize_som = train_som(64, 64, data, 1.0,0.5, 42, number_iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment\n",
    "env = Environment.make('PlantEnv', product_types_df=product_types_df, machines_df=machines_df, orders_df=orders_df,\n",
    "                           simulation_start=SIMULATION_START, priority_rules=priority_rules,\n",
    "                           allocation_rule=select_machine_winq, random_state=42, \n",
    "                       number_states=64 * 64,\n",
    "                           number_actions=8,\n",
    "                           som=som, normalize_som=normalize_som)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q-Learning agent\n",
    "epsilon = Parameter(value=0.25)\n",
    "policy = EpsGreedy(epsilon=epsilon)\n",
    "learning_rate = Parameter(value=0.6)\n",
    "agent = QLearning(env, policy, learning_rate)\n",
    "core = Core(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Q-learning agent\n",
    "# Duration ~ 25 hours\n",
    "# for episode in tqdm(range(100)):\n",
    "#     for random_state in random_states:\n",
    "#         env.random_state = random_state\n",
    "#         for due_date_range in due_date_range_list:\n",
    "#             for number_orders_start in number_orders_start_list:\n",
    "#                 for average_count_new_orders in average_count_new_orders_list:\n",
    "#                     for worker in worker_list:\n",
    "#                         # Change random state before each simulation to increase variety of data\n",
    "#                         env.random_state += 1\n",
    "#                         # Episode using act and observe\n",
    "#                         env.due_date_range = due_date_range\n",
    "#                         env.number_orders_start = number_orders_start\n",
    "#                         env.average_count_new_orders = average_count_new_orders\n",
    "#                         env.worker = worker\n",
    "#                         core.learn(n_episodes=1, n_steps_per_fit=1, render=False, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model for evaluate\n",
    "# Duration ~\n",
    "# reward_list = []\n",
    "# for episode in tqdm(range(1)):\n",
    "#     for random_state in random_states_evaluation:\n",
    "#         env.random_state = random_state\n",
    "#         for due_date_range in due_date_range_list:\n",
    "#             for number_orders_start in number_orders_start_list:\n",
    "#                 for average_count_new_orders in average_count_new_orders_list:\n",
    "#                     for worker in worker_list:\n",
    "#                         # Change random state before each simulation to increase variety of data\n",
    "#                         env.random_state += 1\n",
    "#                         # Episode using act and observe\n",
    "#                         env.due_date_range = due_date_range\n",
    "#                         env.number_orders_start = number_orders_start\n",
    "#                         env.average_count_new_orders = average_count_new_orders\n",
    "#                         env.worker = worker\n",
    "#                         reward_list.extend(core.evaluate(n_episodes=1, render=False, quiet=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as dataframe\n",
    "# results = pd.DataFrame(reward_list, columns = [\"State_before\", \"Action\", \"Reward\", \"State_After\", \"_\", \"__\"])\n",
    "# results.to_csv(\"../data/processed/evaluation/simulation_results_validation_select_q_learning.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Load results\n",
    "results = pd.read_csv(\"../data/processed/evaluation/simulation_results_validation_select_q_learning.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "7454.576187690739"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean reward\n",
    "results[\"Reward\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
