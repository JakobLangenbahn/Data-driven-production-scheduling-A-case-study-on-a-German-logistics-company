{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign jobs directly with proximal policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare programming environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "from tensorforce.agents import Agent\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.simulation import assign_priority_edd, assign_priority_mdd, assign_priority_spt, assign_priority_srpt, assign_priority_lpt, assign_priority_cr, assign_priority_ds, assign_priority_fifo, select_machine_winq\n",
    "from src.models import AssignEnvironment, hyperparameter_tuning_assign_ppo, run_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data required for simulation\n",
    "product_types_df = pd.read_csv(\"../data/external/product_types.csv\")\n",
    "product_types_df = product_types_df[product_types_df.id != 2]\n",
    "with open(r\"../data/interim/sim_data.pickle\", \"rb\") as output_file:\n",
    "    orders_df = pickle.load(output_file)\n",
    "machines_df = pd.read_csv(\"../data/external/machine.csv\")\n",
    "machines_df = machines_df[machines_df.product_type_id != 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for simulation\n",
    "priority_rules = [assign_priority_edd, assign_priority_spt, assign_priority_srpt, assign_priority_fifo, assign_priority_cr,\n",
    "                  assign_priority_mdd, assign_priority_lpt, assign_priority_ds]\n",
    "SIMULATION_START = mktime(datetime(2022, 11, 14, 5, 0, 0).timetuple()) * 1000\n",
    "due_date_range_list = [(3, 10), (5, 14), (7, 21)]\n",
    "number_orders_start_list = [80, 90, 100, 110]\n",
    "average_count_new_orders_list = [80, 90, 100, 110]\n",
    "worker_list =  [40, 50, 60, 70]\n",
    "random_states = [7, 42, 66, 97, 108]\n",
    "random_states_evaluation = [100, 101, 102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.create(\n",
    "    environment=AssignEnvironment(product_types_df, machines_df, orders_df,priority_rule=assign_priority_edd,\n",
    "                 simulation_start=SIMULATION_START, allocation_rule=select_machine_winq, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for ppo learning\n",
    "params_ppo = {\"batch_size\": [5, 10, 20, 30, 50, 100],\n",
    "            \"update_frequency\": [0.25, 0.5, 1.0],\n",
    "            \"discount\": [0.9, 0.7, 0.5, 0.01],\n",
    "            \"return_processing\": [dict(type='exponential_normalization', decay=0.9), None],\n",
    "            \"reward_processing\": [dict(type='exponential_normalization', decay=0.9), None],\n",
    "            \"state_preprocessing\": [dict(type='exponential_normalization', decay=0.9),  None],\n",
    "            \"l2_regularization\": [0.01, 0.0],\n",
    "            \"likelihood_ratio_clipping\": [0.1, 0.2, 0.3],\n",
    "            \"entropy_regularization\": [0.0, 0.01]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute hyperparameter search\n",
    "# tuning_res = hyperparameter_tuning_assign_ppo(environment, params_ppo, 1, due_date_range_list,\n",
    "#                                               number_orders_start_list, average_count_new_orders_list,\n",
    "#                                               worker_list, [42], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results of hyperparameter search\n",
    "# tuning_res.to_csv(\"../data/processed/hyperparameter_search/results_hyperparameter_assign_proximal_policy_optimization.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hyperparameter results\n",
    "tuning_res = pd.read_csv(\"../data/processed/hyperparameter_search/results_hyperparameter_assign_proximal_policy_optimization.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    batch_size  update_frequency  discount  \\\n63           5              0.50      0.01   \n29           5              0.25      0.70   \n54           5              0.50      0.90   \n97          10              0.50      0.01   \n88           5              1.00      0.70   \n..         ...               ...       ...   \n93          20              0.50      0.01   \n14          50              0.50      0.50   \n8           30              1.00      0.01   \n5           10              0.50      0.01   \n7           50              0.25      0.01   \n\n                                    return_processing  \\\n63  {'type': 'exponential_normalization', 'decay':...   \n29                                                NaN   \n54  {'type': 'exponential_normalization', 'decay':...   \n97  {'type': 'exponential_normalization', 'decay':...   \n88  {'type': 'exponential_normalization', 'decay':...   \n..                                                ...   \n93  {'type': 'exponential_normalization', 'decay':...   \n14                                                NaN   \n8                                                 NaN   \n5   {'type': 'exponential_normalization', 'decay':...   \n7                                                 NaN   \n\n                                  state_preprocessing  \\\n63                                                NaN   \n29                                                NaN   \n54                                                NaN   \n97                                                NaN   \n88                                                NaN   \n..                                                ...   \n93                                                NaN   \n14  {'type': 'exponential_normalization', 'decay':...   \n8   {'type': 'exponential_normalization', 'decay':...   \n5   {'type': 'exponential_normalization', 'decay':...   \n7   {'type': 'exponential_normalization', 'decay':...   \n\n                                    reward_processing  l2_regularization  \\\n63                                                NaN               0.00   \n29  {'type': 'exponential_normalization', 'decay':...               0.01   \n54                                                NaN               0.00   \n97                                                NaN               0.00   \n88                                                NaN               0.00   \n..                                                ...                ...   \n93  {'type': 'exponential_normalization', 'decay':...               0.01   \n14                                                NaN               0.00   \n8   {'type': 'exponential_normalization', 'decay':...               0.00   \n5   {'type': 'exponential_normalization', 'decay':...               0.01   \n7   {'type': 'exponential_normalization', 'decay':...               0.00   \n\n    entropy_regularization  likelihood_ratio_clipping  reward_training_mean  \\\n63                    0.00                        0.3            589.612709   \n29                    0.00                        0.3            581.753780   \n54                    0.01                        0.2            558.318754   \n97                    0.00                        0.3            541.876892   \n88                    0.01                        0.2            522.886177   \n..                     ...                        ...                   ...   \n93                    0.00                        0.3             52.286353   \n14                    0.00                        0.1             37.569176   \n8                     0.00                        0.3            -10.527091   \n5                     0.00                        0.1            -38.490446   \n7                     0.01                        0.2            -84.378964   \n\n    reward_training_var  reward_evaluation_mean  reward_evaluation_var  \\\n63         5.034348e+06              554.154325           4.198803e+06   \n29         4.971397e+06              546.616113           4.488806e+06   \n54         3.501242e+06              537.786121           3.714055e+06   \n97         3.763843e+06              559.496317           4.013382e+06   \n88         3.439398e+06              503.081598           3.647436e+06   \n..                  ...                     ...                    ...   \n93         1.858489e+07                2.782588           1.887667e+07   \n14         1.896357e+07               57.479926           1.329789e+07   \n8          1.890367e+07             -691.392437           5.563301e+07   \n5          2.072603e+07             -116.878301           2.276274e+07   \n7          2.256916e+07              265.757806           5.615110e+06   \n\n                                            reward_df  \n63         episode  day        reward due_date_ran...  \n29         episode  day        reward due_date_ran...  \n54         episode  day       reward due_date_rang...  \n97         episode  day       reward due_date_rang...  \n88         episode  day        reward due_date_ran...  \n..                                                ...  \n93         episode  day       reward due_date_rang...  \n14         episode  day       reward due_date_rang...  \n8          episode  day        reward due_date_ran...  \n5          episode  day        reward due_date_ran...  \n7          episode  day        reward due_date_ran...  \n\n[100 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>batch_size</th>\n      <th>update_frequency</th>\n      <th>discount</th>\n      <th>return_processing</th>\n      <th>state_preprocessing</th>\n      <th>reward_processing</th>\n      <th>l2_regularization</th>\n      <th>entropy_regularization</th>\n      <th>likelihood_ratio_clipping</th>\n      <th>reward_training_mean</th>\n      <th>reward_training_var</th>\n      <th>reward_evaluation_mean</th>\n      <th>reward_evaluation_var</th>\n      <th>reward_df</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>63</th>\n      <td>5</td>\n      <td>0.50</td>\n      <td>0.01</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.3</td>\n      <td>589.612709</td>\n      <td>5.034348e+06</td>\n      <td>554.154325</td>\n      <td>4.198803e+06</td>\n      <td>episode  day        reward due_date_ran...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>5</td>\n      <td>0.25</td>\n      <td>0.70</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>0.3</td>\n      <td>581.753780</td>\n      <td>4.971397e+06</td>\n      <td>546.616113</td>\n      <td>4.488806e+06</td>\n      <td>episode  day        reward due_date_ran...</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>5</td>\n      <td>0.50</td>\n      <td>0.90</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>0.2</td>\n      <td>558.318754</td>\n      <td>3.501242e+06</td>\n      <td>537.786121</td>\n      <td>3.714055e+06</td>\n      <td>episode  day       reward due_date_rang...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>10</td>\n      <td>0.50</td>\n      <td>0.01</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.3</td>\n      <td>541.876892</td>\n      <td>3.763843e+06</td>\n      <td>559.496317</td>\n      <td>4.013382e+06</td>\n      <td>episode  day       reward due_date_rang...</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>5</td>\n      <td>1.00</td>\n      <td>0.70</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>0.2</td>\n      <td>522.886177</td>\n      <td>3.439398e+06</td>\n      <td>503.081598</td>\n      <td>3.647436e+06</td>\n      <td>episode  day        reward due_date_ran...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>20</td>\n      <td>0.50</td>\n      <td>0.01</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>0.3</td>\n      <td>52.286353</td>\n      <td>1.858489e+07</td>\n      <td>2.782588</td>\n      <td>1.887667e+07</td>\n      <td>episode  day       reward due_date_rang...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>50</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>NaN</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.1</td>\n      <td>37.569176</td>\n      <td>1.896357e+07</td>\n      <td>57.479926</td>\n      <td>1.329789e+07</td>\n      <td>episode  day       reward due_date_rang...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>30</td>\n      <td>1.00</td>\n      <td>0.01</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.3</td>\n      <td>-10.527091</td>\n      <td>1.890367e+07</td>\n      <td>-691.392437</td>\n      <td>5.563301e+07</td>\n      <td>episode  day        reward due_date_ran...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10</td>\n      <td>0.50</td>\n      <td>0.01</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>0.1</td>\n      <td>-38.490446</td>\n      <td>2.072603e+07</td>\n      <td>-116.878301</td>\n      <td>2.276274e+07</td>\n      <td>episode  day        reward due_date_ran...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>50</td>\n      <td>0.25</td>\n      <td>0.01</td>\n      <td>NaN</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>{'type': 'exponential_normalization', 'decay':...</td>\n      <td>0.00</td>\n      <td>0.01</td>\n      <td>0.2</td>\n      <td>-84.378964</td>\n      <td>2.256916e+07</td>\n      <td>265.757806</td>\n      <td>5.615110e+06</td>\n      <td>episode  day        reward due_date_ran...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 14 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show best performing combination\n",
    "tuning_res.sort_values([\"reward_training_mean\", \"reward_evaluation_mean\"], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No min_value bound specified for state.\n"
     ]
    }
   ],
   "source": [
    "# Define agent based on hyperparameter results\n",
    "agent = Agent.create(\n",
    "    agent='ppo', environment=environment, memory= 10000, batch_size=5,\n",
    "    summarizer=dict(\n",
    "        directory='summaries/assign/ppo',\n",
    "        summaries=[\"action-value\", \"entropy\", \"graph\", \"kl-divergence\", \"loss\", \"parameters\", \"reward\", \"update-norm\",\n",
    "                   \"updates\", \"variables\"]\n",
    "    ),\n",
    "    update_frequency=0.25, learning_rate=0.001, discount=0.9, return_processing={'type': 'exponential_normalization', 'decay': 0.9}, reward_processing=None,\n",
    "    state_preprocessing=None, likelihood_ratio_clipping = 0.3, entropy_regularization = 0.01, exploration=0.2, l2_regularization=0.0,  max_episode_timesteps = 1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent for 100 episodes to train it\n",
    "# Duration ~ 29 hours\n",
    "# rewards_list = run_agent(agent, environment, due_date_range_list,number_orders_start_list,                         average_count_new_orders_list, worker_list, random_states, episodes = 40, evaluate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "# agent.save(directory=\"../models/assign_ppo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %tensorboard --logdir summaries/assign/ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run agent for evaluation\n",
    "# Duration ~ 20 minutes\n",
    "# rewards_list_evaluation = run_agent(agent, environment, due_date_range_list,number_orders_start_list, average_count_new_orders_list, worker_list, random_states_evaluation, episodes = 1, evaluate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# rewards_list_evaluation.to_csv(\"../data/processed/evaluation/simulation_results_validation_assign_ppo.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Load results\n",
    "rewards_list_evaluation = pd.read_csv(\"../data/processed/evaluation/simulation_results_validation_assign_ppo.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "9689.112223796816"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean reward by summing all reward and dividing through the number of days\n",
    "rewards_list_evaluation[\"reward\"].sum() / 17280"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
